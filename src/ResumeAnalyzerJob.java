/**
 * Created by Nitish on 4/14/2017.
 */

import java.io.File;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class ResumeAnalyzerJob {
    public static void main(String[] args) throws Exception {
        Job job = new Job();
        /* Autogenerated initialization. */
        initJob(job);
		/* Custom initialization. */
        initCustom(job);
		/* Tell Task Tracker this is the main */
        job.setJarByClass(ResumeAnalyzerJob.class);
		/*Delete the output folder if it already exists*/
        File index = new File(args[1] + "_intermediate");
        deleteFolder(index);

		/* This is an example of how to set input and output. */
        FileInputFormat.setInputPaths(job, args[0]);
        FileOutputFormat.setOutputPath(job, new Path(args[1] + "_intermediate"));

		/* And finally, we submit the job. */
        job.submit();

        boolean success = job.waitForCompletion(true);

        if (success) {
            /*Job job2 = new Job();
            *//* Autogenerated initialization. *//*
            initJob2(job2);
			*//* Custom initialization. *//*
            initCustom(job2);
			*//* Tell Task Tracker this is the main *//*
            job2.setJarByClass(HomeworkJob.class);

            File index2 = new File(args[1] + "_final");
            deleteFolder(index2);

			*//* This is an example of how to set input and output. *//*
            FileInputFormat.setInputPaths(job2, args[1] + "_intermediate\\part-r-00000");
            FileOutputFormat.setOutputPath(job2, new Path(args[1] + "_final"));

			*//* And finally, we submit the job. *//*
            job2.submit();*/
        }
    }

    public static void deleteFolder(File folder) {
        File[] files = folder.listFiles();
        if (files != null) { // some JVMs return null for empty dirs
            for (File f : files) {
                if (f.isDirectory()) {
                    deleteFolder(f);
                } else {
                    f.delete();
                }
            }
        }
        folder.delete();
    }

    public static void initCustom(Job job) {
    }

    public static void initJob(Job job) {
        job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat.class);
        job.setMapperClass(ParserMapper.class);
        job.getConfiguration().set("mapred.mapper.new-api", "true");
        job.getConfiguration().set("mapred.map.tasks", "3");
        job.setMapOutputKeyClass(org.apache.hadoop.io.Text.class);
        job.setMapOutputValueClass(org.apache.hadoop.io.Text.class);
        job.setPartitionerClass(org.apache.hadoop.mapreduce.lib.partition.HashPartitioner.class);
        job.setReducerClass(ParserReducer.class);
        job.getConfiguration().set("mapred.reducer.new-api", "true");
        job.getConfiguration().set("mapred.reduce.tasks", "2");
        job.setOutputKeyClass(org.apache.hadoop.io.Text.class);
        job.setOutputValueClass(org.apache.hadoop.io.Text.class);
        job.setOutputFormatClass(org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.class);
        job.getConfiguration().set("", "");
    }

    /*public static void initJob2(Job job) {
        job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat.class);
        job.setMapperClass(ParserMapper.class);
        job.getConfiguration().set("mapred.mapper.new-api", "true");
        job.getConfiguration().set("mapred.map.tasks", "3");
        job.setMapOutputKeyClass(org.apache.hadoop.io.Text.class);
        job.setMapOutputValueClass(org.apache.hadoop.io.Text.class);
        job.setPartitionerClass(org.apache.hadoop.mapreduce.lib.partition.HashPartitioner.class);
        job.setReducerClass(ParserReducer.class);
        job.getConfiguration().set("mapred.reducer.new-api", "true");
        job.getConfiguration().set("mapred.reduce.tasks", "2");
        job.setOutputKeyClass(org.apache.hadoop.io.Text.class);
        job.setOutputValueClass(org.apache.hadoop.io.Text.class);
        job.setOutputFormatClass(org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.class);
        job.getConfiguration().set("", "");
    }*/
}
